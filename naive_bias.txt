import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = 'pattern2.csv'
df = pd.read_csv(file_path)

# Convert 'Deaths' column to binary classification
df['Deaths_binary'] = (df['Deaths'] > 0).astype(int)

# Select features and target
selected_features = ['population', 'Cases', 'Deaths']
X = df[selected_features]
y = df['Deaths_binary']  # Target column for Deaths

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets (80:20)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define Na誰ve Bayes model
nb_model = GaussianNB()

# Train the model
nb_model.fit(X_train, y_train)

# Predict on the test set
y_pred_nb = nb_model.predict(X_test)
y_pred_proba_nb = nb_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class

# Print Classification Report for Na誰ve Bayes
print("Classification Report for Na誰ve Bayes:\n", classification_report(y_test, y_pred_nb))

# Plot Confusion Matrix for Test Data
cm_nb = confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Blues", xticklabels=['0', '1'], yticklabels=['0', '1'])
plt.title('Na誰ve Bayes Confusion Matrix (Test Data)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Precision, Recall, F1-Score for different thresholds
thresholds = np.linspace(0, 1, 100)
precisions, recalls, f1_scores = [], [], []

for threshold in thresholds:
    y_pred_threshold = (y_pred_proba_nb > threshold).astype("int32")
    precision_epoch, recall_epoch, f1_score_epoch, _ = precision_recall_fscore_support(y_test, y_pred_threshold, average='binary')
    
    precisions.append(precision_epoch)
    recalls.append(recall_epoch)
    f1_scores.append(f1_score_epoch)

plt.figure(figsize=(12, 8))
plt.plot(thresholds, precisions, label='Precision', color='blue')
plt.plot(thresholds, recalls, label='Recall', color='green')
plt.plot(thresholds, f1_scores, label='F1 Score', color='red')
plt.xlabel('Threshold')
plt.ylabel('Score')
plt.title('Precision, Recall, and F1 Score vs. Threshold')
plt.legend()
plt.grid(True)
plt.show()
